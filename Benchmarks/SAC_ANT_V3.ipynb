{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bcec2e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-17T16:10:30.095838Z",
     "iopub.status.busy": "2023-06-17T16:10:30.095352Z",
     "iopub.status.idle": "2023-06-17T16:13:45.074009Z",
     "shell.execute_reply": "2023-06-17T16:13:45.072264Z"
    },
    "papermill": {
     "duration": 194.989529,
     "end_time": "2023-06-17T16:13:45.077495",
     "exception": false,
     "start_time": "2023-06-17T16:10:30.087966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting templates from packages: 100%\r\n",
      "(Reading database ... 106398 files and directories currently installed.)\r\n",
      "Preparing to unpack .../00-libx11-6_2%3a1.6.9-2ubuntu1.5_amd64.deb ...\r\n",
      "Unpacking libx11-6:amd64 (2:1.6.9-2ubuntu1.5) over (2:1.6.9-2ubuntu1.2) ...\r\n",
      "Selecting previously unselected package libwayland-server0:amd64.\r\n",
      "Preparing to unpack .../01-libwayland-server0_1.18.0-1ubuntu0.1_amd64.deb ...\r\n",
      "Unpacking libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\r\n",
      "Selecting previously unselected package libgbm1:amd64.\r\n",
      "Preparing to unpack .../02-libgbm1_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\r\n",
      "Unpacking libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Selecting previously unselected package libegl-mesa0:amd64.\r\n",
      "Preparing to unpack .../03-libegl-mesa0_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\r\n",
      "Unpacking libegl-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Selecting previously unselected package libegl1:amd64.\r\n",
      "Preparing to unpack .../04-libegl1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libegl1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package xorg-sgml-doctools.\r\n",
      "Preparing to unpack .../05-xorg-sgml-doctools_1%3a1.11-1_all.deb ...\r\n",
      "Unpacking xorg-sgml-doctools (1:1.11-1) ...\r\n",
      "Selecting previously unselected package x11proto-dev.\r\n",
      "Preparing to unpack .../06-x11proto-dev_2019.2-1ubuntu1_all.deb ...\r\n",
      "Unpacking x11proto-dev (2019.2-1ubuntu1) ...\r\n",
      "Selecting previously unselected package x11proto-core-dev.\r\n",
      "Preparing to unpack .../07-x11proto-core-dev_2019.2-1ubuntu1_all.deb ...\r\n",
      "Unpacking x11proto-core-dev (2019.2-1ubuntu1) ...\r\n",
      "Selecting previously unselected package libxau-dev:amd64.\r\n",
      "Preparing to unpack .../08-libxau-dev_1%3a1.0.9-0ubuntu1_amd64.deb ...\r\n",
      "Unpacking libxau-dev:amd64 (1:1.0.9-0ubuntu1) ...\r\n",
      "Selecting previously unselected package libxdmcp-dev:amd64.\r\n",
      "Preparing to unpack .../09-libxdmcp-dev_1%3a1.1.3-0ubuntu1_amd64.deb ...\r\n",
      "Unpacking libxdmcp-dev:amd64 (1:1.1.3-0ubuntu1) ...\r\n",
      "Selecting previously unselected package xtrans-dev.\r\n",
      "Preparing to unpack .../10-xtrans-dev_1.4.0-1_all.deb ...\r\n",
      "Unpacking xtrans-dev (1.4.0-1) ...\r\n",
      "Selecting previously unselected package libpthread-stubs0-dev:amd64.\r\n",
      "Preparing to unpack .../11-libpthread-stubs0-dev_0.4-1_amd64.deb ...\r\n",
      "Unpacking libpthread-stubs0-dev:amd64 (0.4-1) ...\r\n",
      "Selecting previously unselected package libxcb1-dev:amd64.\r\n",
      "Preparing to unpack .../12-libxcb1-dev_1.14-2_amd64.deb ...\r\n",
      "Unpacking libxcb1-dev:amd64 (1.14-2) ...\r\n",
      "Selecting previously unselected package libx11-dev:amd64.\r\n",
      "Preparing to unpack .../13-libx11-dev_2%3a1.6.9-2ubuntu1.5_amd64.deb ...\r\n",
      "Unpacking libx11-dev:amd64 (2:1.6.9-2ubuntu1.5) ...\r\n",
      "Selecting previously unselected package libglx-dev:amd64.\r\n",
      "Preparing to unpack .../14-libglx-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libglx-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package libgl-dev:amd64.\r\n",
      "Preparing to unpack .../15-libgl-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libgl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package libegl-dev:amd64.\r\n",
      "Preparing to unpack .../16-libegl-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libegl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package libgles1:amd64.\r\n",
      "Preparing to unpack .../17-libgles1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package libgles2:amd64.\r\n",
      "Preparing to unpack .../18-libgles2_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libgles2:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package libgles-dev:amd64.\r\n",
      "Preparing to unpack .../19-libgles-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package libopengl0:amd64.\r\n",
      "Preparing to unpack .../20-libopengl0_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libopengl0:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package libopengl-dev:amd64.\r\n",
      "Preparing to unpack .../21-libopengl-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package libglvnd-dev:amd64.\r\n",
      "Preparing to unpack .../22-libglvnd-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\r\n",
      "Unpacking libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Selecting previously unselected package libgl1-mesa-dev:amd64.\r\n",
      "Preparing to unpack .../23-libgl1-mesa-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\r\n",
      "Unpacking libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Selecting previously unselected package libglew2.1:amd64.\r\n",
      "Preparing to unpack .../24-libglew2.1_2.1.0-4_amd64.deb ...\r\n",
      "Unpacking libglew2.1:amd64 (2.1.0-4) ...\r\n",
      "Selecting previously unselected package libglu1-mesa:amd64.\r\n",
      "Preparing to unpack .../25-libglu1-mesa_9.0.1-1build1_amd64.deb ...\r\n",
      "Unpacking libglu1-mesa:amd64 (9.0.1-1build1) ...\r\n",
      "Selecting previously unselected package libglu1-mesa-dev:amd64.\r\n",
      "Preparing to unpack .../26-libglu1-mesa-dev_9.0.1-1build1_amd64.deb ...\r\n",
      "Unpacking libglu1-mesa-dev:amd64 (9.0.1-1build1) ...\r\n",
      "Selecting previously unselected package libglew-dev:amd64.\r\n",
      "Preparing to unpack .../27-libglew-dev_2.1.0-4_amd64.deb ...\r\n",
      "Unpacking libglew-dev:amd64 (2.1.0-4) ...\r\n",
      "Selecting previously unselected package libglfw3:amd64.\r\n",
      "Preparing to unpack .../28-libglfw3_3.3.2-1_amd64.deb ...\r\n",
      "Unpacking libglfw3:amd64 (3.3.2-1) ...\r\n",
      "Selecting previously unselected package patchelf.\r\n",
      "Preparing to unpack .../29-patchelf_0.10-2build1_amd64.deb ...\r\n",
      "Unpacking patchelf (0.10-2build1) ...\r\n",
      "Selecting previously unselected package libosmesa6:amd64.\r\n",
      "Preparing to unpack .../30-libosmesa6_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\r\n",
      "Unpacking libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Selecting previously unselected package libosmesa6-dev:amd64.\r\n",
      "Preparing to unpack .../31-libosmesa6-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\r\n",
      "Unpacking libosmesa6-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Setting up libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\r\n",
      "Setting up libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Setting up libpthread-stubs0-dev:amd64 (0.4-1) ...\r\n",
      "Setting up libopengl0:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up xtrans-dev (1.4.0-1) ...\r\n",
      "Setting up libegl-mesa0:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Setting up libgles2:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up libglew2.1:amd64 (2.1.0-4) ...\r\n",
      "Setting up libegl1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up libx11-6:amd64 (2:1.6.9-2ubuntu1.5) ...\r\n",
      "Setting up xorg-sgml-doctools (1:1.11-1) ...\r\n",
      "Setting up libglu1-mesa:amd64 (9.0.1-1build1) ...\r\n",
      "Setting up libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up patchelf (0.10-2build1) ...\r\n",
      "Setting up libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Setting up x11proto-dev (2019.2-1ubuntu1) ...\r\n",
      "Setting up libglfw3:amd64 (3.3.2-1) ...\r\n",
      "Setting up libxau-dev:amd64 (1:1.0.9-0ubuntu1) ...\r\n",
      "Setting up libxdmcp-dev:amd64 (1:1.1.3-0ubuntu1) ...\r\n",
      "Setting up x11proto-core-dev (2019.2-1ubuntu1) ...\r\n",
      "Setting up libxcb1-dev:amd64 (1.14-2) ...\r\n",
      "Setting up libx11-dev:amd64 (2:1.6.9-2ubuntu1.5) ...\r\n",
      "Setting up libglx-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up libgl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up libegl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up libglu1-mesa-dev:amd64 (9.0.1-1build1) ...\r\n",
      "Setting up libosmesa6-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Setting up libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\r\n",
      "Setting up libglew-dev:amd64 (2.1.0-4) ...\r\n",
      "Setting up libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\r\n",
      "Processing triggers for man-db (2.9.1-1) ...\r\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\n",
      "Collecting mujoco-py<2.2,>=2.1\r\n",
      "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting glfw>=1.4.0\r\n",
      "  Downloading glfw-2.5.9-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (0.29.33)\r\n",
      "Requirement already satisfied: imageio>=2.1.2 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (2.19.3)\r\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (1.21.6)\r\n",
      "Requirement already satisfied: cffi>=1.10 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (1.15.0)\r\n",
      "Requirement already satisfied: fasteners~=0.15 in /opt/conda/lib/python3.7/site-packages (from mujoco-py<2.2,>=2.1) (0.17.3)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1) (2.21)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.7/site-packages (from imageio>=2.1.2->mujoco-py<2.2,>=2.1) (9.1.1)\r\n",
      "Installing collected packages: glfw, mujoco-py\r\n",
      "Successfully installed glfw-2.5.9 mujoco-py-2.1.2.14\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCompiling /opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.pyx because it changed.\n",
      "[1/1] Cythonizing /opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.pyx\n",
      "running build_ext\n",
      "building 'mujoco_py.cymj' extension\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/gl\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/opt/conda/lib/python3.7/site-packages/mujoco_py -I/root/.mujoco/mujoco210/include -I/opt/conda/lib/python3.7/site-packages/numpy/core/include -I/opt/conda/include/python3.7m -c /opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.c -o /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.o -fopenmp -w\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/opt/conda/lib/python3.7/site-packages/mujoco_py -I/root/.mujoco/mujoco210/include -I/opt/conda/lib/python3.7/site-packages/numpy/core/include -I/opt/conda/include/python3.7m -c /opt/conda/lib/python3.7/site-packages/mujoco_py/gl/osmesashim.c -o /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/lib.linux-x86_64-3.7\n",
      "creating /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py\n",
      "gcc -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/cymj.o /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/temp.linux-x86_64-3.7/opt/conda/lib/python3.7/site-packages/mujoco_py/gl/osmesashim.o -L/root/.mujoco/mujoco210/bin -Wl,-R/root/.mujoco/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /opt/conda/lib/python3.7/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_37_linuxcpuextensionbuilder/lib.linux-x86_64-3.7/mujoco_py/cymj.cpython-37m-x86_64-linux-gnu.so -fopenmp\n",
      "Collecting box2d-py\r\n",
      "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m448.6/448.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: box2d-py\r\n",
      "Successfully installed box2d-py-2.3.8\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: gym[Box_2D] in /opt/conda/lib/python3.7/site-packages (0.26.2)\r\n",
      "\u001b[33mWARNING: gym 0.26.2 does not provide the extra 'box_2d'\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: importlib-metadata>=4.8.0 in /opt/conda/lib/python3.7/site-packages (from gym[Box_2D]) (6.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from gym[Box_2D]) (1.21.6)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from gym[Box_2D]) (0.0.8)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[Box_2D]) (2.1.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (3.8.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (4.1.1)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import Math, HTML\n",
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "if not os.path.exists('.mujoco_setup_complete'):\n",
    "  # Get the prereqs\n",
    "  !apt-get -qq update\n",
    "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
    "  # Get Mujoco\n",
    "  !mkdir ~/.mujoco\n",
    "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
    "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
    "  !rm mujoco.tar.gz\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n",
    "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n",
    "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
    "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
    "  !ldconfig\n",
    "  # Install Mujoco-py\n",
    "  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
    "  # run once\n",
    "  !touch .mujoco_setup_complete\n",
    "\n",
    "try:\n",
    "  if _mujoco_run_once:\n",
    "    pass\n",
    "except NameError:\n",
    "  _mujoco_run_once = False\n",
    "if not _mujoco_run_once:\n",
    "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "  try:\n",
    "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
    "  except KeyError:\n",
    "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
    "  try:\n",
    "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  except KeyError:\n",
    "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "  # presetup so we don't see output on first env initialization\n",
    "  import mujoco_py\n",
    "  _mujoco_run_once = True\n",
    "#source of this code block : https://gist.github.com/BuildingAtom/3119ac9c595324c8001a7454f23bf8c8\n",
    "!pip3 install box2d-py\n",
    "!pip3 install gym[Box_2D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6cd95c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T16:13:45.110037Z",
     "iopub.status.busy": "2023-06-17T16:13:45.109018Z",
     "iopub.status.idle": "2023-06-17T16:13:45.116528Z",
     "shell.execute_reply": "2023-06-17T16:13:45.115276Z"
    },
    "papermill": {
     "duration": 0.026806,
     "end_time": "2023-06-17T16:13:45.119052",
     "exception": false,
     "start_time": "2023-06-17T16:13:45.092246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faaed7b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T16:13:45.149999Z",
     "iopub.status.busy": "2023-06-17T16:13:45.149589Z",
     "iopub.status.idle": "2023-06-17T16:13:45.173619Z",
     "shell.execute_reply": "2023-06-17T16:13:45.172598Z"
    },
    "papermill": {
     "duration": 0.042404,
     "end_time": "2023-06-17T16:13:45.175858",
     "exception": false,
     "start_time": "2023-06-17T16:13:45.133454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def push(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mem_cntr\n",
    "\n",
    "def create_log_gaussian(mean, log_std, t):\n",
    "\tquadratic = -((0.5 * (t - mean) / (log_std.exp())).pow(2))\n",
    "\tl = mean.shape\n",
    "\tlog_z = log_std\n",
    "\tz = l[-1] * math.log(2 * math.pi)\n",
    "\tlog_p = quadratic.sum(dim=-1) - log_z.sum(dim=-1) - 0.5 * z\n",
    "\treturn log_p\n",
    "\n",
    "def logsumexp(inputs, dim=None, keepdim=False):\n",
    "\tif dim is None:\n",
    "\t\tinputs = inputs.view(-1)\n",
    "\t\tdim = 0\n",
    "\ts, _ = torch.max(inputs, dim=dim, keepdim=True)\n",
    "\toutputs = s + (inputs - s).exp().sum(dim=dim, keepdim=True).log()\n",
    "\tif not keepdim:\n",
    "\t\toutputs = outputs.squeeze(dim)\n",
    "\treturn outputs\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "\tfor target_param, param in zip(target.parameters(), source.parameters()):\n",
    "\t\ttarget_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "\tfor target_param, param in zip(target.parameters(), source.parameters()):\n",
    "\t\ttarget_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df8b433",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T16:13:45.208580Z",
     "iopub.status.busy": "2023-06-17T16:13:45.207809Z",
     "iopub.status.idle": "2023-06-17T16:13:45.240542Z",
     "shell.execute_reply": "2023-06-17T16:13:45.239066Z"
    },
    "papermill": {
     "duration": 0.052459,
     "end_time": "2023-06-17T16:13:45.243432",
     "exception": false,
     "start_time": "2023-06-17T16:13:45.190973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear6 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xu = torch.cat([state, action], 1)\n",
    "\n",
    "        x1 = F.relu(self.linear1(xu))\n",
    "        x1 = F.relu(self.linear2(x1))\n",
    "        x1 = self.linear3(x1)\n",
    "\n",
    "        x2 = F.relu(self.linear4(xu))\n",
    "        x2 = F.relu(self.linear5(x2))\n",
    "        x2 = self.linear6(x2)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "        # action rescaling\n",
    "        if action_space is None:\n",
    "            self.action_scale = torch.tensor(1.)\n",
    "            self.action_bias = torch.tensor(0.)\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor(\n",
    "                (action_space.high - action_space.low) / 2.)\n",
    "            self.action_bias = torch.FloatTensor(\n",
    "                (action_space.high + action_space.low) / 2.)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super(GaussianPolicy, self).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0efb6fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T16:13:45.275849Z",
     "iopub.status.busy": "2023-06-17T16:13:45.275368Z",
     "iopub.status.idle": "2023-06-17T16:13:45.305319Z",
     "shell.execute_reply": "2023-06-17T16:13:45.304036Z"
    },
    "papermill": {
     "duration": 0.049246,
     "end_time": "2023-06-17T16:13:45.307672",
     "exception": false,
     "start_time": "2023-06-17T16:13:45.258426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SAC(object):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.alpha=0.2\n",
    "        \n",
    "        self.target_entropy = -torch.prod(torch.Tensor(action_space.shape)).item()\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.alpha_optim = optim.Adam([self.log_alpha], lr=3e-4)\n",
    "\n",
    "        self.policy_type = \"Gaussian\"\n",
    "        self.target_update_interval = 1\n",
    "\n",
    "        self.device = torch.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], 256).to(device=self.device)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], 256).to(self.device)\n",
    "        \n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        self.policy = GaussianPolicy(num_inputs, action_space.shape[0], 256, action_space).to(self.device)\n",
    "        self.policy_optim = optim.Adam(self.policy.parameters(), lr=3e-4)\n",
    "            \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if evaluate is False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        # Sample a batch from memory\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size=batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
    "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
    "            next_q_value = reward_batch + mask_batch * self.gamma * (min_qf_next_target)\n",
    "        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
    "        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = ð”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ð”¼st+1~p[V(st+1)]))^2]\n",
    "        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = ð”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ð”¼st+1~p[V(st+1)]))^2]\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
    "\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() # JÏ€ = ð”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "        self.alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optim.step()\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "        return qf1_loss.item(), qf2_loss.item(), policy_loss.item(), alpha_loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60b4cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T16:13:45.339141Z",
     "iopub.status.busy": "2023-06-17T16:13:45.338046Z",
     "iopub.status.idle": "2023-06-17T16:14:02.458537Z",
     "shell.execute_reply": "2023-06-17T16:14:02.456781Z"
    },
    "papermill": {
     "duration": 17.139211,
     "end_time": "2023-06-17T16:14:02.461354",
     "exception": false,
     "start_time": "2023-06-17T16:13:45.322143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d] in /opt/conda/lib/python3.7/site-packages (0.26.2)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]) (0.0.8)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]) (6.0.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]) (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]) (1.21.6)\r\n",
      "Collecting pygame==2.1.0\r\n",
      "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting swig==4.*\r\n",
      "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting box2d-py==2.3.5\r\n",
      "  Downloading box2d_py-2.3.5-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.8.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym[box2d]) (4.1.1)\r\n",
      "Installing collected packages: swig, box2d-py, pygame\r\n",
      "  Attempting uninstall: box2d-py\r\n",
      "    Found existing installation: box2d-py 2.3.8\r\n",
      "    Uninstalling box2d-py-2.3.8:\r\n",
      "      Successfully uninstalled box2d-py-2.3.8\r\n",
      "Successfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.1.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4aaeaf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T16:14:02.497998Z",
     "iopub.status.busy": "2023-06-17T16:14:02.497512Z",
     "iopub.status.idle": "2023-06-17T20:48:30.335883Z",
     "shell.execute_reply": "2023-06-17T20:48:30.334851Z"
    },
    "papermill": {
     "duration": 16467.85967,
     "end_time": "2023-06-17T20:48:30.338765",
     "exception": false,
     "start_time": "2023-06-17T16:14:02.479095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/envs/registration.py:556: UserWarning: \u001b[33mWARN: The environment Ant-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "/opt/conda/lib/python3.7/site-packages/gym/envs/mujoco/mujoco_env.py:191: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  \"This version of the mujoco environments depends \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 10 episodes: 949.487  step5000\n",
      "Evaluation over 10 episodes: 951.622  step10000\n",
      "Evaluation over 10 episodes: 588.258  step15000\n",
      "Evaluation over 10 episodes: 32.416  step20000\n",
      "Evaluation over 10 episodes: 706.363  step25000\n",
      "episode: 100   reward: 613.6889813653929  m :-81.0266048503942 t -81.0266048503942    steps so far:26452\n",
      "Evaluation over 10 episodes: 758.792  step30000\n",
      "Evaluation over 10 episodes: 731.832  step35000\n",
      "Evaluation over 10 episodes: 837.498  step40000\n",
      "Evaluation over 10 episodes: 821.404  step45000\n",
      "Evaluation over 10 episodes: 680.982  step50000\n",
      "Evaluation over 10 episodes: 576.656  step55000\n",
      "Evaluation over 10 episodes: 681.165  step60000\n",
      "Evaluation over 10 episodes: 778.034  step65000\n",
      "Evaluation over 10 episodes: 836.481  step70000\n",
      "Evaluation over 10 episodes: 857.871  step75000\n",
      "Evaluation over 10 episodes: 903.160  step80000\n",
      "Evaluation over 10 episodes: 928.894  step85000\n",
      "Evaluation over 10 episodes: 881.895  step90000\n",
      "Evaluation over 10 episodes: 907.319  step95000\n",
      "Evaluation over 10 episodes: 848.549  step100000\n",
      "Evaluation over 10 episodes: 819.461  step105000\n",
      "Evaluation over 10 episodes: 910.324  step110000\n",
      "Evaluation over 10 episodes: 906.168  step115000\n",
      "Evaluation over 10 episodes: 901.146  step120000\n",
      "episode: 200   reward: 882.5553172950337  m :762.8395064582854 t 340.9064508039457    steps so far:121696\n",
      "Evaluation over 10 episodes: 908.172  step125000\n",
      "Evaluation over 10 episodes: 871.130  step130000\n",
      "Evaluation over 10 episodes: 905.150  step135000\n",
      "Evaluation over 10 episodes: 921.284  step140000\n",
      "Evaluation over 10 episodes: 928.230  step145000\n",
      "Evaluation over 10 episodes: 940.722  step150000\n",
      "Evaluation over 10 episodes: 928.576  step155000\n",
      "Evaluation over 10 episodes: 911.433  step160000\n",
      "Evaluation over 10 episodes: 947.999  step165000\n",
      "Evaluation over 10 episodes: 951.735  step170000\n",
      "Evaluation over 10 episodes: 932.678  step175000\n",
      "Evaluation over 10 episodes: 940.348  step180000\n",
      "Evaluation over 10 episodes: 938.780  step185000\n",
      "Evaluation over 10 episodes: 910.972  step190000\n",
      "Evaluation over 10 episodes: 923.468  step195000\n",
      "Evaluation over 10 episodes: 846.590  step200000\n",
      "Evaluation over 10 episodes: 910.900  step205000\n",
      "Evaluation over 10 episodes: 941.368  step210000\n",
      "Evaluation over 10 episodes: 928.269  step215000\n",
      "episode: 300   reward: 921.613713637239  m :855.5410057862239 t 512.451302464705    steps so far:216809\n",
      "Evaluation over 10 episodes: 896.225  step220000\n",
      "Evaluation over 10 episodes: 955.495  step225000\n",
      "Evaluation over 10 episodes: 948.662  step230000\n",
      "Evaluation over 10 episodes: 894.387  step235000\n",
      "Evaluation over 10 episodes: 941.505  step240000\n",
      "Evaluation over 10 episodes: 951.230  step245000\n",
      "Evaluation over 10 episodes: 962.184  step250000\n",
      "Evaluation over 10 episodes: 943.077  step255000\n",
      "Evaluation over 10 episodes: 810.020  step260000\n",
      "Evaluation over 10 episodes: 852.958  step265000\n",
      "Evaluation over 10 episodes: 922.054  step270000\n",
      "Evaluation over 10 episodes: 906.300  step275000\n",
      "Evaluation over 10 episodes: 929.860  step280000\n",
      "Evaluation over 10 episodes: 962.144  step285000\n",
      "Evaluation over 10 episodes: 945.081  step290000\n",
      "Evaluation over 10 episodes: 859.044  step295000\n",
      "Evaluation over 10 episodes: 944.887  step300000\n",
      "Evaluation over 10 episodes: 946.445  step305000\n",
      "Evaluation over 10 episodes: 920.712  step310000\n",
      "Evaluation over 10 episodes: 825.574  step315000\n",
      "episode: 400   reward: 905.9266013729896  m :912.8512141813394 t 612.5512803938637    steps so far:316809\n",
      "Evaluation over 10 episodes: 970.506  step320000\n",
      "Evaluation over 10 episodes: 942.774  step325000\n",
      "Evaluation over 10 episodes: 980.819  step330000\n",
      "Evaluation over 10 episodes: 770.172  step335000\n",
      "Evaluation over 10 episodes: 951.162  step340000\n",
      "Evaluation over 10 episodes: 856.745  step345000\n",
      "Evaluation over 10 episodes: 753.827  step350000\n",
      "Evaluation over 10 episodes: 883.504  step355000\n",
      "Evaluation over 10 episodes: 959.053  step360000\n",
      "Evaluation over 10 episodes: 945.997  step365000\n",
      "Evaluation over 10 episodes: 860.353  step370000\n",
      "Evaluation over 10 episodes: 902.738  step375000\n",
      "Evaluation over 10 episodes: 921.850  step380000\n",
      "Evaluation over 10 episodes: 759.703  step385000\n",
      "Evaluation over 10 episodes: 948.364  step390000\n",
      "Evaluation over 10 episodes: 969.885  step395000\n",
      "Evaluation over 10 episodes: 807.873  step400000\n",
      "Evaluation over 10 episodes: 967.285  step405000\n",
      "Evaluation over 10 episodes: 951.206  step410000\n",
      "episode: 500   reward: 846.2458530191004  m :879.5631723644004 t 665.9536587879713    steps so far:410938\n",
      "Evaluation over 10 episodes: 958.400  step415000\n",
      "Evaluation over 10 episodes: 913.766  step420000\n",
      "Evaluation over 10 episodes: 1018.891  step425000\n",
      "Evaluation over 10 episodes: 917.560  step430000\n",
      "Evaluation over 10 episodes: 984.654  step435000\n",
      "Evaluation over 10 episodes: 1162.933  step440000\n",
      "Evaluation over 10 episodes: 881.748  step445000\n",
      "Evaluation over 10 episodes: 612.603  step450000\n",
      "Evaluation over 10 episodes: 1068.471  step455000\n",
      "Evaluation over 10 episodes: 965.582  step460000\n",
      "Evaluation over 10 episodes: 859.706  step465000\n",
      "Evaluation over 10 episodes: 573.032  step470000\n",
      "Evaluation over 10 episodes: 1139.695  step475000\n",
      "Evaluation over 10 episodes: 683.510  step480000\n",
      "Evaluation over 10 episodes: 1011.893  step485000\n",
      "Evaluation over 10 episodes: 1450.856  step490000\n",
      "episode: 600   reward: 902.2735130035206  m :892.827463214228 t 703.7659595256811    steps so far:491901\n",
      "Evaluation over 10 episodes: 1252.665  step495000\n",
      "Evaluation over 10 episodes: 1374.925  step500000\n",
      "Evaluation over 10 episodes: 1562.006  step505000\n",
      "Evaluation over 10 episodes: 1528.084  step510000\n",
      "Evaluation over 10 episodes: 1729.037  step515000\n",
      "Evaluation over 10 episodes: 1587.981  step520000\n",
      "Evaluation over 10 episodes: 1565.268  step525000\n",
      "Evaluation over 10 episodes: 1757.248  step530000\n",
      "Evaluation over 10 episodes: 765.580  step535000\n",
      "Evaluation over 10 episodes: 1067.144  step540000\n",
      "Evaluation over 10 episodes: 1386.522  step545000\n",
      "Evaluation over 10 episodes: 1154.600  step550000\n",
      "Evaluation over 10 episodes: 1685.414  step555000\n",
      "Evaluation over 10 episodes: 1833.432  step560000\n",
      "Evaluation over 10 episodes: 1779.505  step565000\n",
      "Evaluation over 10 episodes: 1208.604  step570000\n",
      "Evaluation over 10 episodes: 1385.544  step575000\n",
      "episode: 700   reward: 1132.755141403752  m :1361.5294137319079 t 797.7321672694274    steps so far:579160\n",
      "Evaluation over 10 episodes: 1565.563  step580000\n",
      "Evaluation over 10 episodes: 1971.711  step585000\n",
      "Evaluation over 10 episodes: 1710.320  step590000\n",
      "Evaluation over 10 episodes: 1341.625  step595000\n",
      "Evaluation over 10 episodes: 1546.769  step600000\n",
      "Evaluation over 10 episodes: 1124.862  step605000\n",
      "Evaluation over 10 episodes: 1258.841  step610000\n",
      "Evaluation over 10 episodes: 1065.389  step615000\n",
      "Evaluation over 10 episodes: 1908.732  step620000\n",
      "Evaluation over 10 episodes: 1810.181  step625000\n",
      "Evaluation over 10 episodes: 1239.797  step630000\n",
      "Evaluation over 10 episodes: 1940.614  step635000\n",
      "Evaluation over 10 episodes: 1445.271  step640000\n",
      "Evaluation over 10 episodes: 1197.604  step645000\n",
      "Evaluation over 10 episodes: 2272.508  step650000\n",
      "Evaluation over 10 episodes: 2214.769  step655000\n",
      "Evaluation over 10 episodes: 2067.502  step660000\n",
      "Evaluation over 10 episodes: 1434.904  step665000\n",
      "episode: 800   reward: 1720.8533787728977  m :1564.9343416679546 t 893.6324390692425    steps so far:667060\n",
      "Evaluation over 10 episodes: 2148.573  step670000\n",
      "Evaluation over 10 episodes: 1575.322  step675000\n",
      "Evaluation over 10 episodes: 2244.079  step680000\n",
      "Evaluation over 10 episodes: 2087.292  step685000\n",
      "Evaluation over 10 episodes: 2090.383  step690000\n",
      "Evaluation over 10 episodes: 2114.627  step695000\n",
      "Evaluation over 10 episodes: 2234.415  step700000\n",
      "Evaluation over 10 episodes: 1921.445  step705000\n",
      "Evaluation over 10 episodes: 1646.115  step710000\n",
      "Evaluation over 10 episodes: 1998.409  step715000\n",
      "Evaluation over 10 episodes: 1535.464  step720000\n",
      "Evaluation over 10 episodes: 1836.415  step725000\n",
      "Evaluation over 10 episodes: 1804.646  step730000\n",
      "Evaluation over 10 episodes: 2169.161  step735000\n",
      "Evaluation over 10 episodes: 2004.204  step740000\n",
      "Evaluation over 10 episodes: 1356.598  step745000\n",
      "Evaluation over 10 episodes: 1516.725  step750000\n",
      "Evaluation over 10 episodes: 1848.790  step755000\n",
      "episode: 900   reward: 2340.191246752268  m :1915.6801740491646 t 1007.1932985114561    steps so far:759219\n",
      "Evaluation over 10 episodes: 1291.907  step760000\n",
      "Evaluation over 10 episodes: 2162.850  step765000\n",
      "Evaluation over 10 episodes: 2116.973  step770000\n",
      "Evaluation over 10 episodes: 2555.843  step775000\n",
      "Evaluation over 10 episodes: 2376.005  step780000\n",
      "Evaluation over 10 episodes: 2176.646  step785000\n",
      "Evaluation over 10 episodes: 1443.827  step790000\n",
      "Evaluation over 10 episodes: 2531.430  step795000\n",
      "Evaluation over 10 episodes: 2494.877  step800000\n",
      "Evaluation over 10 episodes: 2408.505  step805000\n",
      "Evaluation over 10 episodes: 2025.341  step810000\n",
      "Evaluation over 10 episodes: 2459.688  step815000\n",
      "Evaluation over 10 episodes: 1823.117  step820000\n",
      "Evaluation over 10 episodes: 2382.068  step825000\n",
      "Evaluation over 10 episodes: 2281.579  step830000\n",
      "Evaluation over 10 episodes: 2551.147  step835000\n",
      "Evaluation over 10 episodes: 2427.535  step840000\n",
      "Evaluation over 10 episodes: 2315.140  step845000\n",
      "episode: 1000   reward: 2764.9862663379586  m :2080.49231591853 t 1114.5232002521632    steps so far:848052\n",
      "Evaluation over 10 episodes: 1936.406  step850000\n",
      "Evaluation over 10 episodes: 2438.942  step855000\n",
      "Evaluation over 10 episodes: 1781.203  step860000\n",
      "Evaluation over 10 episodes: 2606.644  step865000\n",
      "Evaluation over 10 episodes: 2389.121  step870000\n",
      "Evaluation over 10 episodes: 1897.770  step875000\n",
      "Evaluation over 10 episodes: 2094.488  step880000\n",
      "Evaluation over 10 episodes: 2760.060  step885000\n",
      "Evaluation over 10 episodes: 1990.412  step890000\n",
      "Evaluation over 10 episodes: 2877.783  step895000\n",
      "Evaluation over 10 episodes: 2548.255  step900000\n",
      "Evaluation over 10 episodes: 2176.285  step905000\n",
      "Evaluation over 10 episodes: 2145.631  step910000\n",
      "Evaluation over 10 episodes: 2587.849  step915000\n",
      "Evaluation over 10 episodes: 2417.666  step920000\n",
      "Evaluation over 10 episodes: 2423.008  step925000\n",
      "Evaluation over 10 episodes: 2654.328  step930000\n",
      "episode: 1100   reward: 953.7728824344981  m :1965.4522700936268 t 1191.880388419569    steps so far:933671\n",
      "Evaluation over 10 episodes: 2764.073  step935000\n",
      "Evaluation over 10 episodes: 2958.819  step940000\n",
      "Evaluation over 10 episodes: 2600.722  step945000\n",
      "Evaluation over 10 episodes: 2515.795  step950000\n",
      "Evaluation over 10 episodes: 2018.833  step955000\n",
      "Evaluation over 10 episodes: 2404.174  step960000\n",
      "Evaluation over 10 episodes: 2313.866  step965000\n",
      "Evaluation over 10 episodes: 2740.908  step970000\n",
      "Evaluation over 10 episodes: 2352.655  step975000\n",
      "Evaluation over 10 episodes: 2960.310  step980000\n",
      "Evaluation over 10 episodes: 2992.719  step985000\n",
      "Evaluation over 10 episodes: 2655.506  step990000\n",
      "Evaluation over 10 episodes: 2894.684  step995000\n",
      "Evaluation over 10 episodes: 2324.870  step1000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "import pandas as pd\n",
    "import json,os\n",
    "\n",
    "alg=\"SAC\"\n",
    "seed=3\n",
    "envName='Ant-v3'\n",
    "num_steps=1000000\n",
    "evaluationStep=5000\n",
    "printStep=100\n",
    "start_timesteps=10000\n",
    "\n",
    "def eval_policy(policy, env_name,eval_episodes):\n",
    "    eval_env = gym.make(envName)\n",
    "    avg_reward = 0.\n",
    "    for i in range(eval_episodes):\n",
    "        state, _ = eval_env.reset()\n",
    "        done=False\n",
    "        truncated=False\n",
    "        while (not done) and (not truncated):\n",
    "            action = policy.select_action(np.array(state),evaluate=True)\n",
    "            state, reward, done,truncated,_ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "    avg_reward /= eval_episodes\n",
    "    return avg_reward\n",
    "\n",
    "env = gym.make(envName)\n",
    "env.action_space.seed(seed)\n",
    "T.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "agent = SAC(env.observation_space.shape[0], env.action_space,)\n",
    "\n",
    "memory = ReplayMemory(1000000,env.observation_space.shape,env.action_space.shape[0])\n",
    "\n",
    "evaluations = [eval_policy(agent, envName,10)]\n",
    "\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "agent.update_sys = 0\n",
    "base_weight = 0.6\n",
    "ep_reward_list = []\n",
    "\n",
    "variant = dict(algorithm=alg,env=envName)\n",
    "stepss=0\n",
    "for i in range(1,500000000):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    state ,_ = env.reset(seed=seed)\n",
    "    truncated=False\n",
    "    while (not done) and (not truncated):\n",
    "        if stepss < start_timesteps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.select_action(state)\n",
    "        next_state, reward, done,truncated,_ = env.step(action) # Step\n",
    "        episode_steps += 1\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "        stepss+=1\n",
    "        memory.push(state, action, reward, next_state, not(done))\n",
    "        state = next_state\n",
    "        if len(memory) > 10000:\n",
    "            agent.update_parameters(memory, 100, updates)\n",
    "            updates += 1\n",
    "        if(total_numsteps%evaluationStep)==0:\n",
    "            avg_reward=eval_policy(agent,envName,10)\n",
    "            evaluations.append(avg_reward)\n",
    "            print(f\"Evaluation over {10} episodes: {avg_reward:.3f}  step{stepss}\")\n",
    "    if(stepss>num_steps):\n",
    "            break\n",
    "    ep_reward_list.append(episode_reward)\n",
    "    if (i%printStep)==0:\n",
    "        if i<100:\n",
    "            print(f\"episode: {i}   reward: {episode_reward}  avg so far:{sum(ep_reward_list)/len(ep_reward_list)} steps so far:{total_numsteps}\")\n",
    "        else:\n",
    "            print(f\"episode: {i}   reward: {episode_reward}  m :{sum(ep_reward_list[-100:])/len(ep_reward_list[-100:])} t {sum(ep_reward_list)/len(ep_reward_list)}    steps so far:{total_numsteps}\")\n",
    "\n",
    "if not os.path.exists(f\"./data/{envName}/{alg}/seed{seed}\"):\n",
    "    os.makedirs(f'./data/{envName}/{alg}/seed{seed}')\n",
    "with open(f'./data/{envName}/{alg}/seed{seed}/variant.json', 'w') as outfile:\n",
    "    json.dump(variant,outfile)\n",
    "data = np.array(evaluations)\n",
    "df = pd.DataFrame(data=data,columns=[\"Average Return\"]).reset_index()\n",
    "df['Timesteps'] = df['index'] * evaluationStep\n",
    "df['env'] = envName\n",
    "df['algorithm_name'] = alg\n",
    "df.to_csv(f'./data/{envName}/{alg}/seed{seed}/progress.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16693.210945,
   "end_time": "2023-06-17T20:48:31.941409",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-17T16:10:18.730464",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
