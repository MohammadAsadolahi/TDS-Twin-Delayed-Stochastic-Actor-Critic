{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3038cf2c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-26T10:25:56.371197Z",
     "iopub.status.busy": "2024-06-26T10:25:56.370835Z",
     "iopub.status.idle": "2024-06-26T10:26:20.255309Z",
     "shell.execute_reply": "2024-06-26T10:26:20.254155Z"
    },
    "papermill": {
     "duration": 23.89256,
     "end_time": "2024-06-26T10:26:20.257985",
     "exception": false,
     "start_time": "2024-06-26T10:25:56.365425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[mujoco] in /opt/conda/lib/python3.10/site-packages (0.29.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (2.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (4.9.0)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (0.0.4)\r\n",
      "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\r\n",
      "  Downloading mujoco-3.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (2.33.1)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio>=2.14.1->gymnasium[mujoco]) (9.5.0)\r\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\r\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.6.0)\r\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\r\n",
      "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (5.4 kB)\r\n",
      "Collecting pyopengl (from mujoco>=2.3.3->gymnasium[mujoco])\r\n",
      "  Downloading PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2024.3.1)\r\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.1.1)\r\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.17.0)\r\n",
      "Downloading mujoco-3.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyopengl, glfw, mujoco\r\n",
      "Successfully installed glfw-2.7.0 mujoco-3.1.6 pyopengl-3.1.7\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import Math, HTML\n",
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "import copy\n",
    "import pandas as pd\n",
    "import json,os\n",
    "!pip install gymnasium[mujoco]\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde754e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T10:26:20.270848Z",
     "iopub.status.busy": "2024-06-26T10:26:20.269564Z",
     "iopub.status.idle": "2024-06-26T10:26:20.281139Z",
     "shell.execute_reply": "2024-06-26T10:26:20.280262Z"
    },
    "papermill": {
     "duration": 0.020146,
     "end_time": "2024-06-26T10:26:20.283338",
     "exception": false,
     "start_time": "2024-06-26T10:26:20.263192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7879c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T10:26:20.294576Z",
     "iopub.status.busy": "2024-06-26T10:26:20.294183Z",
     "iopub.status.idle": "2024-06-26T10:26:20.303213Z",
     "shell.execute_reply": "2024-06-26T10:26:20.302100Z"
    },
    "papermill": {
     "duration": 0.01746,
     "end_time": "2024-06-26T10:26:20.305766",
     "exception": false,
     "start_time": "2024-06-26T10:26:20.288306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_dims[0] + n_actions, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "    \n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = self.fc1(T.cat([state, action], dim=1))\n",
    "        q = F.relu(q)\n",
    "        q = self.fc2(q)\n",
    "        q = F.relu(q)\n",
    "        q = self.q(q)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec387abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T10:26:20.317324Z",
     "iopub.status.busy": "2024-06-26T10:26:20.316957Z",
     "iopub.status.idle": "2024-06-26T10:26:20.327618Z",
     "shell.execute_reply": "2024-06-26T10:26:20.326568Z"
    },
    "papermill": {
     "duration": 0.019019,
     "end_time": "2024-06-26T10:26:20.329902",
     "exception": false,
     "start_time": "2024-06-26T10:26:20.310883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, fc1_dims, fc2_dims,n_actions,max_action):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.max_action=max_action\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.ln1 = nn.LayerNorm(self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.ln2 = nn.LayerNorm(self.fc2_dims)\n",
    "        self.sigma = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        prob =self.fc1(state)\n",
    "        prob=self.ln1(prob)\n",
    "        prob = F.relu(prob)\n",
    "        prob =self.fc2(prob)\n",
    "        prob=self.ln2(prob)\n",
    "        prob = F.relu(prob)\n",
    "\n",
    "        mu =self.max_action * T.tanh(self.mu(prob))\n",
    "        sigma =F.sigmoid(self.sigma(prob)).clamp(min=0.1*self.max_action, max=1*self.max_action)\n",
    "        return mu,sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe721463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T10:26:20.341292Z",
     "iopub.status.busy": "2024-06-26T10:26:20.340967Z",
     "iopub.status.idle": "2024-06-26T10:26:20.375998Z",
     "shell.execute_reply": "2024-06-26T10:26:20.374963Z"
    },
    "papermill": {
     "duration": 0.043728,
     "end_time": "2024-06-26T10:26:20.378449",
     "exception": false,
     "start_time": "2024-06-26T10:26:20.334721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, alpha, beta, input_dims, tau, action_space_high,action_space_low,\n",
    "            gamma=0.99, actor_update_interval=2,n_actions=2, max_size=1000000, layer1_size=400,\n",
    "            layer2_size=300, batch_size=100):\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "#         self.to(self.device)\n",
    "        \n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.actor = ActorNetwork(input_dims, layer1_size,layer2_size, n_actions,action_space_high[0])\n",
    "        self.actor_optimizer=optim.Adam(params=self.actor.parameters(), lr=alpha)\n",
    "        self.critic_1 = CriticNetwork(input_dims, layer1_size,layer2_size,n_actions)\n",
    "        self.critic_1_optimizer=optim.Adam(params=self.critic_1.parameters(), lr=beta)\n",
    "        self.critic_2 = CriticNetwork(input_dims, layer1_size,layer2_size,n_actions)\n",
    "        self.critic_2_optimizer=optim.Adam(params=self.critic_2.parameters(), lr=beta)\n",
    "\n",
    "        self.target_actor = ActorNetwork(input_dims, layer1_size,layer2_size, n_actions,action_space_high[0])\n",
    "        self.target_critic_1 = CriticNetwork(input_dims, layer1_size,layer2_size,n_actions)\n",
    "        self.target_critic_2 = CriticNetwork(input_dims, layer1_size,layer2_size,n_actions)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.action_space_high = action_space_high\n",
    "        self.action_space_low = action_space_low\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        self.update_actor_iter =  actor_update_interval\n",
    "        self.learn_step_cntr = 0\n",
    "        self.time_step = 0\n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor(observation, dtype=T.float).to(self.device)\n",
    "        self.actor.eval()\n",
    "        with T.no_grad():\n",
    "            mu, sigma = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        noise = (T.randn_like(mu) * sigma).clamp(-0.5*self.action_space_high[0], 0.5*self.action_space_high[0])\n",
    "        action=T.clamp((mu + noise),self.action_space_low[0],self.action_space_high[0])\n",
    "        return action.numpy(),mu.numpy()\n",
    "      \n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, new_state, done = \\\n",
    "                self.memory.sample_buffer(self.batch_size)\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.device)             \n",
    "        done = T.tensor(done, dtype=T.float).to(self.device)\n",
    "        state_ = T.tensor(new_state, dtype=T.float).to(self.device)\n",
    "        state = T.tensor(state, dtype=T.float).to(self.device)\n",
    "        action = T.tensor(action, dtype=T.float).to(self.device)\n",
    "        \n",
    "        with T.no_grad():\n",
    "            mu, sigma = self.target_actor.forward(state_)\n",
    "            noise = (T.randn_like(action) * (0.2*self.action_space_high[0])).clamp(-0.5*self.action_space_high[0], 0.5*self.action_space_high[0])\n",
    "            target_actions = (mu + noise).clamp(self.action_space_low [0],self.action_space_high[0])\n",
    "            q1_ = T.squeeze(agent.target_critic_1.forward(state_, target_actions))\n",
    "            q2_ = T.squeeze(agent.target_critic_2.forward(state_, target_actions))\n",
    "\n",
    "        q1 = T.squeeze(agent.critic_1.forward(state, action))\n",
    "        q2 =T.squeeze(agent.critic_2.forward(state, action))\n",
    "        \n",
    "        critic_value_ = T.min(T.squeeze(q1_),T.squeeze(q2_))\n",
    "        target = reward + self.gamma * (1 - done) * (critic_value_)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        q1_loss = F.mse_loss(q1,target)\n",
    "        q2_loss = F.mse_loss(q2,target)\n",
    "        q1_loss.backward()\n",
    "        q2_loss.backward()\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        self.learn_step_cntr += 1\n",
    "        if self.learn_step_cntr % self.update_actor_iter != 0:\n",
    "            return\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        mean, std = self.actor.forward(state)\n",
    "        action_distribution=T.distributions.Normal(mean.detach(), std) \n",
    "#         actor_min_Q_loss = self.critic_1.forward(state, mean)\n",
    "        actor_min_Q_loss = T.min(self.critic_1.forward(state, mean),self.critic_2.forward(state, mean))\n",
    "        actor_mu_loss = T.mean(T.sum(- action_distribution.log_prob(mean) * actor_min_Q_loss.detach(),axis=0) - actor_min_Q_loss)\n",
    "        actor_mu_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_1_params = self.critic_1.named_parameters()\n",
    "        critic_2_params = self.critic_2.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_1_params = self.target_critic_1.named_parameters()\n",
    "        target_critic_2_params = self.target_critic_2.named_parameters()\n",
    "\n",
    "        critic_1_state_dict = dict(critic_1_params)\n",
    "        critic_2_state_dict = dict(critic_2_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_actor_state_dict = dict(target_actor_params)\n",
    "        target_critic_1_state_dict = dict(target_critic_1_params)\n",
    "        target_critic_2_state_dict = dict(target_critic_2_params)\n",
    "\n",
    "        for name in critic_1_state_dict:\n",
    "            critic_1_state_dict[name] = tau*critic_1_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_critic_1_state_dict[name].clone()\n",
    "\n",
    "        for name in critic_2_state_dict:\n",
    "            critic_2_state_dict[name] = tau*critic_2_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_critic_2_state_dict[name].clone()\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                    (1-tau)*target_actor_state_dict[name].clone()\n",
    "\n",
    "        self.target_critic_1.load_state_dict(critic_1_state_dict)\n",
    "        self.target_critic_2.load_state_dict(critic_2_state_dict)\n",
    "        self.target_actor.load_state_dict(actor_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b83b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T10:26:20.390511Z",
     "iopub.status.busy": "2024-06-26T10:26:20.390079Z",
     "iopub.status.idle": "2024-06-26T13:57:13.868083Z",
     "shell.execute_reply": "2024-06-26T13:57:13.866975Z"
    },
    "papermill": {
     "duration": 12653.487011,
     "end_time": "2024-06-26T13:57:13.870741",
     "exception": false,
     "start_time": "2024-06-26T10:26:20.383730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 10 episodes: -6.488  step5000\n",
      "Evaluation over 10 episodes: -87.289  step10000\n",
      "Evaluation over 10 episodes: 175.387  step15000\n",
      "Evaluation over 10 episodes: 592.834  step20000\n",
      "Evaluation over 10 episodes: 720.179  step25000\n",
      "Evaluation over 10 episodes: 805.990  step30000\n",
      "Evaluation over 10 episodes: 513.073  step35000\n",
      "Evaluation over 10 episodes: 822.330  step40000\n",
      "Evaluation over 10 episodes: 675.054  step45000\n",
      "Evaluation over 10 episodes: 827.230  step50000\n",
      "Evaluation over 10 episodes: 702.396  step55000\n",
      "Evaluation over 10 episodes: 642.801  step60000\n",
      "Evaluation over 10 episodes: 739.908  step65000\n",
      "Evaluation over 10 episodes: 646.275  step70000\n",
      "Evaluation over 10 episodes: 892.423  step75000\n",
      "Evaluation over 10 episodes: 885.381  step80000\n",
      "Evaluation over 10 episodes: 918.244  step85000\n",
      "Evaluation over 10 episodes: 724.387  step90000\n",
      "Evaluation over 10 episodes: 886.012  step95000\n",
      "Evaluation over 10 episodes: 661.868  step100000\n",
      "Evaluation over 10 episodes: 925.630  step105000\n",
      "Evaluation over 10 episodes: 1031.331  step110000\n",
      "Evaluation over 10 episodes: 930.985  step115000\n",
      "Evaluation over 10 episodes: 969.696  step120000\n",
      "Evaluation over 10 episodes: 1114.504  step125000\n",
      "episode: 200   reward: 815.503625160447  m :677.9386827502732 t 472.97994920290773:128275    steps so far:128275\n",
      "Evaluation over 10 episodes: 849.462  step130000\n",
      "Evaluation over 10 episodes: 790.421  step135000\n",
      "Evaluation over 10 episodes: 939.148  step140000\n",
      "Evaluation over 10 episodes: 1152.537  step145000\n",
      "Evaluation over 10 episodes: 1628.326  step150000\n",
      "Evaluation over 10 episodes: 1494.743  step155000\n",
      "Evaluation over 10 episodes: 1654.035  step160000\n",
      "Evaluation over 10 episodes: 1586.451  step165000\n",
      "Evaluation over 10 episodes: 1968.647  step170000\n",
      "Evaluation over 10 episodes: 1047.816  step175000\n",
      "Evaluation over 10 episodes: 2173.007  step180000\n",
      "Evaluation over 10 episodes: 614.899  step185000\n",
      "Evaluation over 10 episodes: 1858.363  step190000\n",
      "Evaluation over 10 episodes: 2159.148  step195000\n",
      "Evaluation over 10 episodes: 2248.649  step200000\n",
      "Evaluation over 10 episodes: 2066.884  step205000\n",
      "Evaluation over 10 episodes: 3114.420  step210000\n",
      "Evaluation over 10 episodes: 1045.147  step215000\n",
      "Evaluation over 10 episodes: 866.258  step220000\n",
      "Evaluation over 10 episodes: 2783.420  step225000\n",
      "Evaluation over 10 episodes: 2560.103  step230000\n",
      "Evaluation over 10 episodes: 3219.579  step235000\n",
      "Evaluation over 10 episodes: 3081.310  step240000\n",
      "Evaluation over 10 episodes: 3121.222  step245000\n",
      "Evaluation over 10 episodes: 597.824  step250000\n",
      "Evaluation over 10 episodes: 2275.314  step255000\n",
      "Evaluation over 10 episodes: 2316.874  step260000\n",
      "Evaluation over 10 episodes: 3187.328  step265000\n",
      "Evaluation over 10 episodes: 3093.755  step270000\n",
      "Evaluation over 10 episodes: 2782.959  step275000\n",
      "Evaluation over 10 episodes: 3010.999  step280000\n",
      "episode: 400   reward: 2256.4591805373207  m :2083.1978245676423 t 1062.4921650263873:284703    steps so far:284703\n",
      "Evaluation over 10 episodes: 2394.006  step285000\n",
      "Evaluation over 10 episodes: 3961.638  step290000\n",
      "Evaluation over 10 episodes: 2309.369  step295000\n",
      "Evaluation over 10 episodes: 3430.368  step300000\n",
      "Evaluation over 10 episodes: 3641.190  step305000\n",
      "Evaluation over 10 episodes: 1500.962  step310000\n",
      "Evaluation over 10 episodes: 4037.948  step315000\n",
      "Evaluation over 10 episodes: 3316.421  step320000\n",
      "Evaluation over 10 episodes: 4521.801  step325000\n",
      "Evaluation over 10 episodes: 4268.183  step330000\n",
      "Evaluation over 10 episodes: 3888.531  step335000\n",
      "Evaluation over 10 episodes: 3800.776  step340000\n",
      "Evaluation over 10 episodes: 4211.562  step345000\n",
      "Evaluation over 10 episodes: 3248.399  step350000\n",
      "Evaluation over 10 episodes: 4040.699  step355000\n",
      "Evaluation over 10 episodes: 2522.255  step360000\n",
      "Evaluation over 10 episodes: 3585.314  step365000\n",
      "Evaluation over 10 episodes: 2993.776  step370000\n",
      "Evaluation over 10 episodes: 3766.317  step375000\n",
      "Evaluation over 10 episodes: 3924.803  step380000\n",
      "Evaluation over 10 episodes: 4507.685  step385000\n",
      "Evaluation over 10 episodes: 4210.675  step390000\n",
      "Evaluation over 10 episodes: 4308.578  step395000\n",
      "Evaluation over 10 episodes: 2602.920  step400000\n",
      "Evaluation over 10 episodes: 4092.707  step405000\n",
      "Evaluation over 10 episodes: 5091.915  step410000\n",
      "Evaluation over 10 episodes: 4610.654  step415000\n",
      "Evaluation over 10 episodes: 5080.880  step420000\n",
      "Evaluation over 10 episodes: 4388.867  step425000\n",
      "Evaluation over 10 episodes: 4691.688  step430000\n",
      "Evaluation over 10 episodes: 4939.308  step435000\n",
      "Evaluation over 10 episodes: 4906.998  step440000\n",
      "Evaluation over 10 episodes: 5145.120  step445000\n",
      "Evaluation over 10 episodes: 4243.927  step450000\n",
      "episode: 600   reward: 5125.054111735902  m :3519.030857627463 t 1827.7463783126495:452727    steps so far:452727\n",
      "Evaluation over 10 episodes: 5037.861  step455000\n",
      "Evaluation over 10 episodes: 4668.436  step460000\n",
      "Evaluation over 10 episodes: 4681.931  step465000\n",
      "Evaluation over 10 episodes: 5050.814  step470000\n",
      "Evaluation over 10 episodes: 5289.966  step475000\n",
      "Evaluation over 10 episodes: 4965.416  step480000\n",
      "Evaluation over 10 episodes: 5347.672  step485000\n",
      "Evaluation over 10 episodes: 5335.726  step490000\n",
      "Evaluation over 10 episodes: 5047.331  step495000\n",
      "Evaluation over 10 episodes: 4797.480  step500000\n",
      "Evaluation over 10 episodes: 5158.473  step505000\n",
      "Evaluation over 10 episodes: 4989.342  step510000\n",
      "Evaluation over 10 episodes: 5201.040  step515000\n",
      "Evaluation over 10 episodes: 5324.457  step520000\n",
      "Evaluation over 10 episodes: 5383.555  step525000\n",
      "Evaluation over 10 episodes: 5440.949  step530000\n",
      "Evaluation over 10 episodes: 5297.862  step535000\n",
      "Evaluation over 10 episodes: 5269.140  step540000\n",
      "Evaluation over 10 episodes: 5029.744  step545000\n",
      "Evaluation over 10 episodes: 5384.542  step550000\n",
      "Evaluation over 10 episodes: 5460.569  step555000\n",
      "Evaluation over 10 episodes: 5575.866  step560000\n",
      "Evaluation over 10 episodes: 5339.969  step565000\n",
      "Evaluation over 10 episodes: 5255.394  step570000\n",
      "Evaluation over 10 episodes: 5313.752  step575000\n",
      "Evaluation over 10 episodes: 5616.912  step580000\n",
      "Evaluation over 10 episodes: 5577.510  step585000\n",
      "Evaluation over 10 episodes: 5589.660  step590000\n",
      "Evaluation over 10 episodes: 5476.287  step595000\n",
      "Evaluation over 10 episodes: 4909.722  step600000\n",
      "Evaluation over 10 episodes: 4724.480  step605000\n",
      "Evaluation over 10 episodes: 5244.015  step610000\n",
      "Evaluation over 10 episodes: 5201.192  step615000\n",
      "Evaluation over 10 episodes: 3990.650  step620000\n",
      "Evaluation over 10 episodes: 4529.215  step625000\n",
      "Evaluation over 10 episodes: 5041.317  step630000\n",
      "episode: 800   reward: 5273.592438118822  m :4494.17634671429 t 2473.314718406197:634814    steps so far:634814\n",
      "Evaluation over 10 episodes: 5383.455  step635000\n",
      "Evaluation over 10 episodes: 5437.647  step640000\n",
      "Evaluation over 10 episodes: 5568.028  step645000\n",
      "Evaluation over 10 episodes: 5365.622  step650000\n",
      "Evaluation over 10 episodes: 5546.161  step655000\n",
      "Evaluation over 10 episodes: 5571.613  step660000\n",
      "Evaluation over 10 episodes: 5194.611  step665000\n",
      "Evaluation over 10 episodes: 5537.306  step670000\n",
      "Evaluation over 10 episodes: 5196.642  step675000\n",
      "Evaluation over 10 episodes: 4775.717  step680000\n",
      "Evaluation over 10 episodes: 4829.054  step685000\n",
      "Evaluation over 10 episodes: 5385.923  step690000\n",
      "Evaluation over 10 episodes: 5608.263  step695000\n",
      "Evaluation over 10 episodes: 5655.278  step700000\n",
      "Evaluation over 10 episodes: 5043.311  step705000\n",
      "Evaluation over 10 episodes: 5473.120  step710000\n",
      "Evaluation over 10 episodes: 5574.619  step715000\n",
      "Evaluation over 10 episodes: 5821.879  step720000\n",
      "Evaluation over 10 episodes: 5383.535  step725000\n",
      "Evaluation over 10 episodes: 5612.707  step730000\n",
      "Evaluation over 10 episodes: 5662.976  step735000\n",
      "Evaluation over 10 episodes: 5717.289  step740000\n",
      "Evaluation over 10 episodes: 5136.646  step745000\n",
      "Evaluation over 10 episodes: 5464.708  step750000\n",
      "Evaluation over 10 episodes: 5054.028  step755000\n",
      "Evaluation over 10 episodes: 5385.282  step760000\n",
      "Evaluation over 10 episodes: 5774.577  step765000\n",
      "Evaluation over 10 episodes: 5425.228  step770000\n",
      "Evaluation over 10 episodes: 5634.988  step775000\n",
      "Evaluation over 10 episodes: 5251.859  step780000\n",
      "Evaluation over 10 episodes: 5695.674  step785000\n",
      "Evaluation over 10 episodes: 5598.880  step790000\n",
      "Evaluation over 10 episodes: 5709.247  step795000\n",
      "Evaluation over 10 episodes: 5578.653  step800000\n",
      "Evaluation over 10 episodes: 5646.711  step805000\n",
      "Evaluation over 10 episodes: 5644.315  step810000\n",
      "Evaluation over 10 episodes: 5249.450  step815000\n",
      "episode: 1000   reward: 5464.199658786127  m :4569.34724374843 t 2917.306433880442:815422    steps so far:815422\n",
      "Evaluation over 10 episodes: 5799.326  step820000\n",
      "Evaluation over 10 episodes: 5382.989  step825000\n",
      "Evaluation over 10 episodes: 5806.489  step830000\n",
      "Evaluation over 10 episodes: 5717.642  step835000\n",
      "Evaluation over 10 episodes: 5667.892  step840000\n",
      "Evaluation over 10 episodes: 5223.556  step845000\n",
      "Evaluation over 10 episodes: 5551.443  step850000\n",
      "Evaluation over 10 episodes: 4953.196  step855000\n",
      "Evaluation over 10 episodes: 5812.496  step860000\n",
      "Evaluation over 10 episodes: 5869.796  step865000\n",
      "Evaluation over 10 episodes: 5877.068  step870000\n",
      "Evaluation over 10 episodes: 5186.641  step875000\n",
      "Evaluation over 10 episodes: 5768.598  step880000\n",
      "Evaluation over 10 episodes: 5274.299  step885000\n",
      "Evaluation over 10 episodes: 5630.734  step890000\n",
      "Evaluation over 10 episodes: 5295.976  step895000\n",
      "Evaluation over 10 episodes: 5342.171  step900000\n",
      "Evaluation over 10 episodes: 5810.713  step905000\n",
      "Evaluation over 10 episodes: 5671.654  step910000\n",
      "Evaluation over 10 episodes: 5841.706  step915000\n",
      "Evaluation over 10 episodes: 5858.380  step920000\n",
      "Evaluation over 10 episodes: 5863.192  step925000\n",
      "Evaluation over 10 episodes: 5411.250  step930000\n",
      "Evaluation over 10 episodes: 5838.122  step935000\n",
      "Evaluation over 10 episodes: 5727.886  step940000\n",
      "Evaluation over 10 episodes: 5429.037  step945000\n",
      "Evaluation over 10 episodes: 5820.590  step950000\n",
      "Evaluation over 10 episodes: 5797.148  step955000\n",
      "Evaluation over 10 episodes: 5888.858  step960000\n",
      "Evaluation over 10 episodes: 5860.227  step965000\n",
      "Evaluation over 10 episodes: 5893.931  step970000\n",
      "Evaluation over 10 episodes: 5781.392  step975000\n",
      "Evaluation over 10 episodes: 6036.768  step980000\n",
      "Evaluation over 10 episodes: 5961.340  step985000\n",
      "Evaluation over 10 episodes: 5870.822  step990000\n",
      "Evaluation over 10 episodes: 5857.652  step995000\n",
      "Evaluation over 10 episodes: 5928.603  step1000000\n"
     ]
    }
   ],
   "source": [
    "algorithm_name=\"TDS\"\n",
    "enviroment_name='Ant-v4'#'Pendulum-v1'#'MountainCarContinuous-v0'\n",
    "seed=4\n",
    "start_timesteps=10000\n",
    "def policy_evaluation(agent, enviroment_name,episodes=10):\n",
    "    evaluation_env = gym.make(enviroment_name)\n",
    "    average_reward = 0.\n",
    "    for _ in range(episodes):\n",
    "        state, _ = evaluation_env.reset()\n",
    "        done=False\n",
    "        truncuated=False\n",
    "        while (not done) and (not truncuated):\n",
    "            _,action = agent.choose_action(np.array(state))\n",
    "            state, reward, done, truncuated,_ = evaluation_env.step(action)\n",
    "            average_reward += reward\n",
    "    average_reward /= episodes\n",
    "    return average_reward\n",
    "\n",
    "env = gym.make(enviroment_name)\n",
    "env.action_space.seed(seed)\n",
    "T.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "agent = Agent(alpha=3e-4, beta=3e-4, \n",
    "            input_dims=env.observation_space.shape, tau=0.005,\n",
    "            action_space_high=env.action_space.high,action_space_low=env.action_space.low,batch_size=100, layer1_size=256, layer2_size=256,\n",
    "            n_actions=env.action_space.shape[0])\n",
    "evaluations = [policy_evaluation(agent,enviroment_name)]\n",
    "average_rewards=[]\n",
    "total_rewards=[]\n",
    "steps=0\n",
    "for ep in range(1,10000000000):\n",
    "    done=False\n",
    "    state,_=env.reset(seed=seed)\n",
    "    rewards=0\n",
    "    episode_timesteps=0\n",
    "    truncuated=False\n",
    "    while (not done) and (not truncuated):\n",
    "        episode_timesteps+=1\n",
    "        if steps < start_timesteps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action,_=agent.choose_action(state)\n",
    "        agent.learn()\n",
    "        state_,reward,done,truncuated,info=env.step(action)\n",
    "        agent.remember(state,action,reward,state_,done)\n",
    "        rewards+=reward\n",
    "        steps+=1\n",
    "        state=state_\n",
    "        if(steps%5000)==0:\n",
    "            evaluation_reward=policy_evaluation(agent, enviroment_name)\n",
    "            evaluations.append(evaluation_reward)\n",
    "            print(f\"Evaluation over {10} episodes: {evaluation_reward:.3f}  step{steps}\")\n",
    "    total_rewards.append(rewards)\n",
    "    average_rewards.append(sum(total_rewards)/len(total_rewards))\n",
    "    if(steps>1000000):\n",
    "        break\n",
    "    if (ep%200==0):\n",
    "        if ep<100:\n",
    "            print(f\"episode: {ep}   reward: {rewards}  avg so far:{average_rewards[-1]} steps so far:{steps}\")\n",
    "        else:\n",
    "            print(f\"episode: {ep}   reward: {rewards}  m :{sum(total_rewards[-100:])/len(total_rewards[-100:])} t {average_rewards[-1]}:{steps}    steps so far:{steps}\")\n",
    "    \n",
    "variant = dict(algorithm=algorithm_name,env=enviroment_name,)\n",
    "if not os.path.exists(f\"./data/{enviroment_name}/{algorithm_name}/seed{seed}\"):\n",
    "    os.makedirs(f'./data/{enviroment_name}/{algorithm_name}/seed{seed}')\n",
    "with open(f'./data/{enviroment_name}/{algorithm_name}/seed{seed}/variant.json', 'w') as outfile:\n",
    "    json.dump(variant,outfile)\n",
    "data = np.array(evaluations)\n",
    "df = pd.DataFrame(data=data,columns=[\"Average Return\"]).reset_index()\n",
    "df['Timesteps'] = df['index'] * 5000\n",
    "df['env'] = enviroment_name\n",
    "df['algorithm_name'] = algorithm_name\n",
    "df.to_csv(f'./data/{enviroment_name}/{algorithm_name}/seed{seed}/progress.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12682.735131,
   "end_time": "2024-06-26T13:57:16.208259",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-26T10:25:53.473128",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
